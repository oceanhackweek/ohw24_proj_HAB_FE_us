---
title: "PACE with R, first Steps"
format: 
  html:
    self-contained: true
    toc: true
---


```{r}
#| label: general packages

library(rnaturalearth) # Shape/country/coastline shapefiles
library(ncdf4) # Netcdf opening and subsetting
library(sf)    # spatial dataframes, for points/polygons etc.
library(stars) # spatial arrays, for gridded spatial information
library(terra)
```



# PACE Satellite Data with R

This is a follow along of what trying to access PACE data looks like if you are just trying to google your way into locating it. Target audience is users that may be ccomfortable with R and working with raster or Netcdf files, but has not used NASA datasets.


## Setting up Earthdata ACCESS

NASA PACE Satellite data is made accessible through the earthdata distribution platform powered by amazon AWS.

This service requires setting up Earthdata Login Credentials.

The [earthdatalogin package](https://github.com/boettiger-lab/earthdatalogin) can be used to login with credentials created on [NASA Earthdata](https://www.earthdata.nasa.gov/).

This chunk of code will prompt you to enter username and password information on its first use, and then store this information securely in a hidden file `.netrc`.

```{r}
# Access credentials
# install.packages("earthdatalogin")
library(earthdatalogin)

# Set up netrc credential file
earthdatalogin::edl_netrc()
```


### URL Discovery

Using [earthdatalogin]() we have established credential access. We're still missing the URL discovery funcitonality of [earthaccess]()

One way to explore available data in the browser is with the Earthdata:

https://search.earthdata.nasa.gov 


## Using python for S3 Granule Discovery

Unfortunately, at the moment the dataset granule discovery functionality is only available with the python library `earthaccess`

One way to take advantage of this would be to make a python code chunk, and use this library 
to discover/query available dataset granules.

```{python}
#| eval: false

# import earthaccess
# 
# # 1. Login
# earthaccess.login(persist=True)
# 
# # 2. Search for Ocean Color Index Data
# results = earthaccess.search_datasets(instrument="oci")
# for item in results:
#     summary = item.summary()
#     print(summary["short-name"])

```

The URL's can be printed this way:

```{python}
#| eval: false

# # Searching by short name for PACE chl
# results = earthaccess.search_data(
#     short_name="PACE_OCI_L3M_CHL_NRT"
# )
# 
# # We can look at the urls
# [result.data_links() for result in results]
```

## Using R for S3 Granule Discovery

For R users the process is very similar, but actively being developed.

I found these helpful links when googling for: "earthdata access from R" that capture the challenges from a very recent past:

[OpenScapes earthdata: python r handoff](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/in-development/earthdata-python-r-handoff.html)


# Constructing URLs & Opening Granules in R

The "easiest" way to find+open the data woulb be to use `earthdatalogin` in a similar way to `earthaccess` to find search and discover, then take that information over to R.

A helpful guide on how to do this can be found [here.](https://nmfs-opensci.github.io/EDMW-EarthData-Workshop-2024/tutorials/r/1-earthdatalogin.html)

Search
```{r}
# Why no edl_search?!?

#' Search for data products using the EarthData API
#'
#' **NOTE**: Use as a fallback method only! Users are strongly encouraged
#'  to rely on the STAC endpoints for NASA EarthData, as shown in the
#'  package vignettes.  STAC is a widely used metadata standard by both
#'  NASA and many other providers, and can be searched using the feature-rich
#'  `rstac` package.  STAC return items can be more easily parsed as well.
#'
#' @param short_name dataset short name e.g. ATL08
#' @param version dataset version
#' @param doi DOI for a dataset
#' @param daac NSIDC or PODAAC
#' @param provider particular to each DAAC, e.g. POCLOUD, LPDAAC etc.
#' @param temporal c("yyyy-mm-dd", "yyyy-mm-dd")
#' @param bounding_box c(lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat)
#' @param page_size maximum number of results to return per query.
#' @param ... additional query parameters
#' @param recurse If a query returns more than page_size results, should
#'   we make recursive calls to return all results?
#' @param parse_results logical, default TRUE. Calls [edl_extract_urls()]
#'  to determine url links to data objects.  Set to FALSE to return
#'  the full API response object, but be wary of large object sizes
#'  when search returns many results.
#' @inheritParams edl_netrc
#' @export
#' @return A character vector of data URLs matching the search criteria,
#' if `parse_results = TRUE` (default).  Otherwise, returns a response object
#'  of the returned search information if `parse_results = FALSE`.
#' @examplesIf interactive()
#'
#' items <- edl_search(short_name = "MUR-JPL-L4-GLOB-v4.1",
#'                    temporal = c("2002-01-01", "2021-12-31"),
#'                    recurse = TRUE,
#'                    parse_urls = TRUE)
#'
#' urls <- edl_extract_urls(items)
#'
edl_search <- function(short_name = NULL,
                       version = NULL,
                       doi = NULL,
                       daac = NULL,
                       provider = NULL,
                       temporal = NULL,
                       bounding_box = NULL,
                       page_size = 2000,
                       recurse = TRUE,
                       parse_results = TRUE,
                       username = default("user"),
                       password = default("password"),
                       netrc_path = edl_netrc_path(),
                       cookie_path = edl_cookie_path(),
                       ...) {

  #token <- earthdatalogin::edl_set_token(set_env_var = FALSE,
  #                                       prompt_for_netrc = FALSE)


  edl_netrc(username = username,
            password = password,
            netrc_path = netrc_path,
            cookie_path = cookie_path,
            cloud_config = FALSE)
  netrc_config <-
    httr::config(netrc = TRUE,
                 netrc_file = netrc_path,
                 cookiefile = cookie_path,
                 cookiejar = cookie_path)

  query <- list(short_name = short_name,
                temporal = paste(as.character(temporal), collapse=","),
                version = version,
                doi = doi,
                daac = daac,
                provider = provider,
                bounding_box = bounding_box,
                page_size = page_size,
                ...)
  query <- purrr::compact(query)

  url <- "https://cmr.earthdata.nasa.gov/search/granules"
  resp <- httr::GET(url,  query = query, netrc_config)
  httr::stop_for_status(resp)

  entry <- httr::content(resp, "parsed")$feed$entry
  resp_header <- httr::headers(resp)
  continue <- resp_header[["cmr-search-after"]]
  #i <- 1


  if(recurse){
    while(!is.null(continue)) {

      resp <- httr::GET(url,
                query = query,
                netrc_config,
                httr::add_headers("CMR-Search-After" = continue))

      more_entries <- httr::content(resp, "parsed")$feed$entry
      entry <- c(entry, more_entries)

      resp_header <- httr::headers(resp)
      continue <- resp_header[["cmr-search-after"]]

    }
  }

  if (!parse_results) {
    return(  structure(entry, class = "cmr_items") )
  }

  edl_extract_urls(entry)
}






```



```{r}
# devtools::install_github("boettiger-lab/earthdatalogin")


short_name <- "PACE_OCI_L3M_CHL_NRT"
tbox <- c("2023-01-16", "2020-12-16")

# Search:
results <- edl_search(
    short_name = short_name,
    version = "4.1",
    temporal = tbox
)

results[1:3]
```



### Granule Discovery in R

Using python in an R tutorial also feels like cheating. So here is an attempt to avoid it.

There exists an online paper trail detailing an old deprecated workflow using `httr`
 to hit the earthdata api and return amazon s3 information to access the data.
 
 https://urs.earthdata.nasa.gov/documentation/for_users/data_access/r 
 
 The original code is below:


```{r}
#| label: httr script
#| eval: false

# Set up R
# You may need to install the httr package.
# install.packages("httr")
library(httr)

netrc_path <- "/path/to/.netrc"
cookie_path <- "/path/to/.urs_cookies"
downloaded_file_path <- "/path/to/filename"

# Before using the script
#Set up your ~/.netrc file as listed here: https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+cURL+And+Wget
set_config(config(followlocation=1,netrc=1,netrc_file=netrc_path,cookie=cookie_path,cookiefile=cookie_path,cookiejar=cookie_path))
httr::GET(url = "https://disc2.gesdisc.eosdis.nasa.gov/data/TRMM_RT/TRMM_3B42RT_Daily.7/2000/03/3B42RT_Daily.20000301.7.nc4",
                  write_disk(downloaded_file_path, overwrite = TRUE))
```


This code could likely be updated using the `httr2` package to provide similar functionality to `earthaccess`.

Details on the earthdata api can be found here:
https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#general-request-details 

```{r}
#install.packages("httr2")
library(httr2)
```



```{r}
# # Another S3 package:
# install.packages("aws.s3")
```






```{r}
# Open URL
url <- "https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T56JKT.2023246T235950.v2.0/HLS.L30.T56JKT.2023246T235950.v2.0.SAA.tif"

# PACE URL from search.earthdata
url <- "https://obdaac-tea.earthdatacloud.nasa.gov/ob-cumulus-prod-public/PACE_OCI.20240826.L3m.DAY.CHL.V2_0.chlor_a.0p1deg.NRT.nc"

# Use terra to open it
terra::rast(url)
# Use ncdf4
ncdf4::nc_open(url)
```


### Opening Granules in R

```{r}

```


